# @package _global_

# global parameters
device: cuda            # TODO
debug: True
deterministic: False
no_workers: -1
seed: 0
comment: ''
# network
net:
  type: ""
  no_hidden: -1
  no_blocks: -1
  data_dim: 0                    # Should be overridden automatically
  dropout: 0.0
  dropout_in: 0.0
  dropout_type: Dropout2d        # TODO
  norm: ""
  nonlinearity: ""
  block_width_factors: [0.0, ]
  block:
    type: default
    prenorm: True
# kernels
kernel:
  type: ""
  no_hidden: -1
  no_layers: -1
  omega_0: 0.0
  bias: True
  size: "same"
  chang_initialize: True
  norm: Identity
  nonlinearity: Identity
  init_spatial_value: 1.0   # Only != 1.0 if FlexConvs are used.
# mask
mask:
  type: ''
  init_value: -1.
  threshold: -1.
  dynamic_cropping: False
  temperature: 0.0        # For sigmoid mask
  learn_mean: False
# convolutions
conv:
  type: ""
  causal: False
  use_fft: False
  bias: True
  padding: "same"
  stride: 1
  cache: False          # TODO
# datamodules
dataset:
  name: ''
  data_dir: '/data'
  data_type: 'default'
  augment: False
  params:
    permuted: False       # For permuted sMNIST
    noise_padded: False   # For noise-padded CIFAR10
    grayscale: False      # For LRA-Image dataset
    memory_size: -1       # For copy memory problem
    mfcc: False           # For MFCC pre-processing on SpeechCommands
    drop_rate: 0.0        # For irregular SpeechCommands and CharTrajetories
    metric: 'MAE'
    resolution: 32        # Used for PathFinder
# training
train:
  do: True
  mixed_precision: False
  epochs: -1
  batch_size: -1
  grad_clip: 0.0
  max_epochs_no_improvement: 100
  track_grad_norm: -1 # -1 for no tracking. TODO
  accumulate_grad_steps: 1 # Accumulate gradient over different batches. TODO
  distributed: False # TODO
  num_nodes: -1
  avail_gpus: -1 # TODO
# optimizer
optimizer:
  name: Adam
  lr: 0.0
  mask_lr_ratio: 1.
  momentum: -1.
  nesterov: False
  weight_decay: 0.0
# scheduler
scheduler:
  name: ''
  decay_steps: -1
  factor: -1.0
  patience: -1
  warmup_epochs: 0
  mode: 'max'
# testing
test:
  batch_size_multiplier: 1
  before_train: False
# wandb logging
wandb:
  project: main
  entity: ccnn
# checkpoint
pretrained:
  load: False
  alias: 'best' #Either best or last
  filename: ""
# hooks; function: application
hooks_enabled: False